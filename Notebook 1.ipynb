{"cells":[{"cell_type":"markdown","source":"# Big Data Team 8","metadata":{"tags":[],"cell_id":"c88ff39dcba3414a9e337dc7c849823f","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":1},"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"Our names are Ehran, Princewell and Lenn and in this report, we will explain the steps we took to come up with a solution for the image recognition project for our deep learning course.","metadata":{"tags":[],"cell_id":"ce1d25268d5b4975bbcc08a7fe221001","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":7},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Since Ehran already had made a model based on felines which suited the requirements for this course, we decided to start from this. Doing so gave us the chance to include more extras than needed. We ended up adding a total of four extras. This includes benchmarking, assessment using ROC and AUC metrics, a Keras model and an API endpoint using Flask.","metadata":{"tags":[],"cell_id":"23c97b60-049a-453c-ad6a-a4c9b420c2df","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":10},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"In this document we will go over our general model as well as those four extras.","metadata":{"tags":[],"cell_id":"335ba569-71ab-43f3-be57-d22ab8483dc6","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":13},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Data set - Types of cats","metadata":{"tags":[],"cell_id":"e4610330867f4bf28b261ffed1ea7e2a","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":16},"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"For this project we utilized a data set that had been scraped by one of our members previously. It consists of roughly 2000 images, comprised of big cats. We focused our model on 5 types of cats: Tiger, Lion, Cheetah, Leopard and the normal house cat. Multiple images that we used were duplicates to allow for data augmentation within the code. One thing to note is that there is a  tiny amount of bias against the leopard since many of the images that were collected turned out to be not images of leopards but of other types of cats such as the cheetah and therefore we had to remove quite a few of these.","metadata":{"tags":[],"cell_id":"3c3994a1-6640-408c-8350-cff938773d5c","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":22},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Below is the python scraper that was used to achieve the data set. This scraper has not been adjusted since the previous assignment for which it was used.","metadata":{"tags":[],"cell_id":"5f2642bb11664fbb949db6c7cc695d1d","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":25},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"import selenium\nimport time\nimport requests\nimport threading\nimport os\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.common.by import By\n\nDRIVER_PATH = \"11. DL - Introduction to Deep Learning\\\\resources\\chromedriver.exe\"\nsearch_terms = ['Cat', 'Cheetah', 'Lion', 'Leopard', 'Tiger']\nthread_list = []\nquery_string = \"https://www.pexels.com/search/{s}/\"\nfor i in search_terms:\n    os.mkdir(\"11. DL - Introduction to Deep Learning\\\\resources\\DL_DataFolders\\DL_{0}\".format(i)) \ndata_set_list = os.listdir(\"11. DL - Introduction to Deep Learning\\\\resources\\DL_DataFolders\") \nservice = Service(DRIVER_PATH)\n\ndef scrape_images(datafolder, query_word):\n    service.start()\n    driver = webdriver.Remote(service.service_url)\n    driver.get(query_string.format(s=(query_word)))\n    time.sleep(2)\n    src = []\n\n    for z in range(6):\n        # Scroll down the body of the web page and load the images.\n        driver.execute_script(\"window.scrollBy(0,1500);\")\n        time.sleep(2)\n        # Find the images.\n        imgResults = driver.find_elements(By.CLASS_NAME,\"MediaCard_image__ljFAl\")\n        \n        # Access and store the scr list of image url's.\n        for img in imgResults:\n            src.append(img.get_attribute('src'))\n        \n\n    \n    time.sleep(2)\n    # Retrieve and download the images.    \n    for i in range(500):   \n        r = requests.get(str(src[i]),stream = True).content\n        with open(\"11. DL - Introduction to Deep Learning\\\\resources\\DL_DataFolders\\{0}\\{1}{2}.jpg\".format(datafolder, query_word, i),'wb') as f:\n                f.write(r)\n        \n\n    driver.quit()\n\nfor i in range(len(data_set_list)):\n    thread_list.append(threading.Thread(target=scrape_images, args=(data_set_list[i], search_terms[i])))\n    thread_list[i].start()\n    \n\nfor i in range(len(data_set_list)):\n    thread_list[i].join()\n","metadata":{"tags":[],"cell_id":"72c48029a0b04eabae071aeca624b101","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":28},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These images have then been evaluated for wrong images which were removed. Afterwards the folder structure was also renamed to make future work with it easier. After the completion of numerous training cycles we adjusted the last wrongly classified images that slipped through the cracks previously.","metadata":{"tags":[],"cell_id":"87b34a4c7b8b4c769a1caebc3cf3cdeb","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":34},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Modeling using fastai","metadata":{"tags":[],"cell_id":"0ba411fdc36c409ea4aa0cd7527a7f6d","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":37},"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"Fastai is a deep learning library which provides us with high level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, providing researcher with low-level components that can be mixed and matched to build new approaches. ","metadata":{"tags":[],"cell_id":"cf45d5af-9708-44f0-92c3-2bdd989a97b8","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":88},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Loading the data","metadata":{"tags":[],"cell_id":"efb057620fdf477280fed6d47573fac2","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":16,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":91},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"The data to fit our model needs to be wrapped in a wrapper class called 'DataLoaders'. The wrapper class takes whatever objects we pass to it which is usually a 'train' and a 'valid'  objects. In other words, we need DataLoaders as a wrapper around our training and validation data, so other fastai functions can call upon them.","metadata":{"tags":[],"cell_id":"2d871360-081a-40ee-846d-2307b3ea4c02","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":94},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Fastai data block API","metadata":{"tags":[],"cell_id":"a7aac26a-8897-454a-b819-e2ac8af81cd9","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":21,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":103},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Since we're going to use a custom dataset in the DataLoaders object, we need to provide fastai info on the kind of data we'll be working with, how to get all the data items, how to label these items and how to create the validation set. This was done using the 'data block API'","metadata":{"tags":[],"cell_id":"808803c5-98e9-4e7f-ad4b-ef1d0e5b3c43","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":106},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"cats = DataBlock(\n    # A turple to specify what types we want for the independent and dependent variables\n    blocks=(ImageBlock, CategoryBlock), \n    # Function below takes a path and returns a list of all the images in that path\n    get_items=get_image_files,\n    # Random splitting of our dataset \n    splitter=RandomSplitter(valid_pct=0.2, seed=1),\n    # Independent variable =x, dependent  = y\n    get_y=parent_label,\n    # Transforming images to the same size and add some augmentation\n    item_tfms=Resize(128))\n    batch_tfms=aug_transforms(size=224, min_scale=0.75))\n # Data loader been fed with the path   \ndls = cats.dataloaders('/content/gdrive/MyDrive/resources/DL_DataFolders_cleaned/train')","metadata":{"tags":[],"cell_id":"e63b7146b27e4f9f93b0ecc8c60aae20","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":112},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating and training the model","metadata":{"tags":[],"cell_id":"9484ac8c7a3b43a89b835dbab8c46b0d","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":31,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":118},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"With the datasets and DataLoaders defined, we'll define our model. Pytorch provides several world class CNN's pretrained on Imagenet, We'll use resent34 model pretrained on Imagenet and only as a feature extractor. So we'll train the fully connected layer of the network.","metadata":{"tags":[],"cell_id":"b293728b-63e9-4a28-a6bf-1d3e93f7294b","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":121},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# The learning rate finder\nresnet34_just_right = vision_learner(dls, resnet34, metrics=error_rate)\nresnet34_just_right.lr_find()","metadata":{"tags":[],"cell_id":"30a98849f06545c5a2a91e94bff8cf3a","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The learning rate finder will do a quick search using the chosen architecture and data, to try to find the best learning rate. Since the learning rate is very significant when training a model, we have to make sure it is right. If we have a low learning rate, there will be a lot of iterations to train the model and we may eventually have problems with overfitting because we give it too much time and the model will have a chance at memorizing.","metadata":{"tags":[],"cell_id":"28b114e01def485489b33b972c5ab265","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"italic":true},"toCodePoint":24,"fromCodePoint":3}],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"resnet_adv = vision_learner(dls, resnet34, metrics=error_rate)\nresnet_adv.fit_one_cycle(3, 3e-3)","metadata":{"tags":[],"cell_id":"d69c6a974fd54fd5b4f728d60f24a5a5","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":127},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A vision_learner is the module that defines the cnn_learner method to easily get a model suitable for transfer learning. The cnn_learner method helps to automatically get a pretrained model from a given architecture with a custom head that is suitable for your data.","metadata":{"tags":[],"cell_id":"33d6829cbaa8494d8805978694e352f1","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"italic":true},"toCodePoint":60,"fromCodePoint":48},{"type":"marks","marks":{"italic":true},"toCodePoint":137,"fromCodePoint":125}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":133},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Since we're doing transfer learning, the DataLoader and the pretrained model resnet34 is passed as an argument to the vision_learner. And we're going to use the fit_one_cycle method due to its better performance in speed and accuracy. It uses large cyclical learning rates to train models significantly quicker and with a higher accuracy. What happens is it start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training. The number of epochs = 3 and the learning rate = 0.003(3e-3) is passed as arguments to the method.","metadata":{"tags":[],"cell_id":"5654a674-7a39-42ef-beb2-719d15a4df0d","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":133},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"The learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward the minimum of a loss function, and an epoch is an iteration over the whole dataset. ","metadata":{"tags":[],"cell_id":"b5284658-9a21-4047-a36c-3d266383d369","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"italic":true},"toCodePoint":17,"fromCodePoint":3},{"type":"marks","marks":{"italic":true},"toCodePoint":182,"fromCodePoint":176}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":133},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Unfreezing the resnet pretrained weight\nresnet_adv.unfreeze()","metadata":{"tags":[],"cell_id":"e77cbe8a70a24553a45c8c52a4e5e96d","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the entire network for a couple of epochs\nresnet_adv.fit_one_cycle(6, 1e-5)","metadata":{"tags":[],"cell_id":"4f645f30c58f45b0b285302d248f0803","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save the model","metadata":{"tags":[],"cell_id":"b88263cbeaf848ddafbb559c75cdbac6","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Export model\nresnet_adv.export('cats.pkl')","metadata":{"tags":[],"cell_id":"cd219d0878e84d4eb9a1de6ca5869843","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Deploying on Streamlit","metadata":{"tags":[],"cell_id":"bb38cb3095384e2aa80e73c6dc59b876","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":46},"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"Streamlit is an open source python library that makes it easy to turn data science, computer vision, NLP, and so forth into apps. Providing extensive documentation, Streamlit allows us to deploy previously difficult to code projects in a matter of minutes. Streamlit can be ran locally on our own machines but we chose to also deploy our app online using the new service of Streamlit cloud. Streamlit cloud is a rather new service and when we deployed the base versions of our app to it we never ran into any problems. However, this changed when we tried to upload the model training part since it requires the use of data augmentation within its dataloaders. This in turn causes the cloud version of Streamlit to run out of memory and reload, stopping any possible interaction. We therefore had to remove this functionality from our online deployed app but it still seems to function correctly on local.","metadata":{"tags":[],"cell_id":"254c871a-7548-4785-888f-f90d2b6c87cf","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":52},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"We used Streamlit to deploy not only our model and the ability to predict different images but we also provided our data set in an easy to use format to perform an EDA and the ability to train a new model using different advanced settings. Streamlit apps can be deployed locally by using the active Shell and simply typing `streamlit run app.py` while in the folder with said Python file. For Streamlit cloud we had to sign up to share.streamlit.io and create a new project where we pointed the project to our github page. Streamlit cloud runs the app by using the appointed .py file within our github and a requirements.txt file that tells Streamlit which dependencies to install. Within this requirements.txt we can also tell Streamlit which specific versions of the imports to use. ","metadata":{"tags":[],"cell_id":"043db08a-ed44-4611-ac75-adb4df6f18ab","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":55},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Streamlit allows us to use simple configuration options to set up a sidebar navigation, give the page a title and apply any other necessary small addition.","metadata":{"tags":[],"cell_id":"0954b7b9d71e4ad4b39a3563b212aa1d","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":85},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# Setting page configurations such as title and layout\nst.set_page_config(\n    page_title=\"Image Classification\",\n    page_icon=\"ðŸ§Š\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\")\n# Creating the navigation method which is always activated upon starting of the Streamlit app\ndef navigation():\n    #1. as slidebar menu\n    with st.sidebar:\n        # We create a new object in which we place different attributes that Streamlit requires to further\n        # build the navigation. Upon clicking on the options which we prescribe inside of the option_menu\n        # it will change the selected variable\n        selected = option_menu(\n            menu_title= \"Big Data\",\n            options = [\"Classifier\", \"EDA\", \"Google Teachable Machine\", \"New Image Trainer\"],\n            # We are also capable of giving specific Streamlit icons to the different navigation options\n            icons=['upload', 'graph-down'],\n            menu_icon=\"cast\", default_index=0\n        )\n    # It will fire the selected function, which if nothing is selected will \n    # be the main app at the beginning\n    if selected == \"Classifier\":\n        main_app()\n    if selected == \"EDA\":\n        eda()\n    if selected == \"Google Teachable Machine\":\n        googlemachine()\n    if selected == \"New Image Trainer\":\n        fastai_training()","metadata":{"tags":[],"cell_id":"ed95b00bdd4c485cb9b2595c2b2b90ac","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":79},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Streamlit functions with the use of methods, for each page it has a different method that is activated when the corresponding navigation button is activated. One notable exception however is the first method which is immediately called, in our case main_app(). The main app is our classic image classification which gives the user a button to upload an image where we will then keep it in a variable. We create two columns using st.colums(2). Streamlit allows us to modify these columns as objects with their own methods which will then apply our modifications to the page. Upon uploading an image it will display the uploaded image together with the predicted class and the probability that it is actually that class.","metadata":{"tags":[],"cell_id":"5c69e2e8-c4b1-47e3-86a0-fd962452e940","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":76},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"def main_app():\n    st.header('Image Classification')\n    st.subheader('Model trained with Fastai')\n    # Setting up the path file system to use windowspath type, this also helps us use \n    # githubs folder structure.\n    plt = platform.system()\n    if plt == 'Windows': pathlib.PosixPath = pathlib.WindowsPath\n    # Load the previously uploaded fastai model which was saved in a pickle file\n    res_model = load_learner(pathlib.Path()/'cats.pkl')\n    # Keep the saved file in a variable while also only allowing image files\n    uploaded_file = st.file_uploader(\"Upload Files\",type=['png','jpeg', 'jpg'])\n    # Creating two columns\n    col1,col2 = st.columns(2)\n    # 'Opening' the first column and utilizing this activated column to replace original information\n    with col1:\n        # Remove the previous image (if there is a previous image)\n        display_image = st.empty()\n        # If there is no uploaded file it will return to showing the beginning information\n        if not uploaded_file:\n            return  display_image.info(\"Choose a file to upload, only type: png, jpg, jpeg \")\n        # If there is an uploaded file we will use the PIL library to open the uploaded image\n        # and afterwards display it onto the page\n        else:\n            uploaded_file = PILImage.create((uploaded_file))\n            display_image.image(uploaded_file.to_thumb(500,500), caption='Image Upload')\n        # Saving the prediction probability of the uploaded file that has \n        # been predicted by our learner model\n        pred, pred_idx, probs = res_model.predict(uploaded_file)\n    with col2:\n        # Upon success of the previous functions it will show the predicted class\n        # and prediction probability\n        st.success(f'Prediction: {pred} ')\n        st.info(f'Probability: {probs[pred_idx]:.04f}')","metadata":{"tags":[],"cell_id":"1ae19b1e4c7a439cb5103681a28e3c59","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":97},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The next method we created is the one for our EDA, Exploratory Data Analysis. The same idea transpired here except that instead of loading a resnet model here we go through our data set and place it on our Streamlit app. We can load all of the images fairly straightforwardly using two for loops. We do this by first taking the names of our labels/classes by listing our directory folders which we have aptly named by our classes. Afterwards we find the images by creating the image names with the class name and path to our data set. Afterwards we apply a filter to separate the different classes' images.","metadata":{"tags":[],"cell_id":"0e2c2f718b8c4f0b869a166f3d60e12b","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":136},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"As part of our EDA we calculate the average resolution too by using the .size[] method on our images and calculating the average of all. Afterwards we will create multiple 'tabs', 5 in total to fit the amount of classes that we have. Inside of the tabs we then display the amount of images of the specified class, with the amount being chosen using a Streamlit slider.","metadata":{"tags":[],"cell_id":"76a42293-e53e-4b3b-b3b4-e3e777bbf15c","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":136},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"def eda():\n    st.header('Exploratory data analysis')\n\n    # List the path to our data set in github\n    data_path = 'cats/'\n    # Instantiating arrays so that we might save our labels and images\n    img_list = []\n    labels = []\n    # Fill in the labels & img_list lists\n    for class_name in os.listdir(data_path):\n        if class_name not in labels:\n            labels.append(class_name)\n        img_dir = data_path + class_name + \"/\"\n        for img_filename in os.listdir(img_dir):\n            img_path = img_dir + img_filename\n            img_list.append([img_path, class_name])\n    # We create a filter to get the label part of the sublist (eg: [[img_path, label], ...]\n    def get_filtered_list(filter: str, list: list = img_list):\n        return [x[0] for x in list if x[1] == filter]\n    # We create a method to calculate the average resolution for a given list of images\n    def get_average_img_resolution(images: list):\n        widths = []\n        heights = []\n        # Use a loop to get the height and width of all of the images\n        for img in images:\n            im = Image.open(img)\n            # Appending the size items width and height) into the previously made array\n            widths.append(im.size[0])\n            heights.append(im.size[1])\n\n        avg_width = round(sum(widths) / len(widths))\n        avg_height = round(sum(heights) / len(heights))\n        return [avg_width, avg_height]\n    # Creating 5 tabs, equal to the amount of classes we have\n    tab1, tab2, tab3, tab4, tab5 = st.tabs(labels)\n    # A tab for each of the labels\n    for index, tab in enumerate([tab1, tab2, tab3, tab4, tab5]):\n        # With activating the tab we are able to write the amount of samples within the data set\n        # and the chosen amount to be displayed. Afterwards we will display the images of\n        # the chose class\n        with tab:\n            images = get_filtered_list(labels[index])\n            total_imgs = len(images)\n            # We get the average image resolution of the images that pertain to the chosen class\n            avg_w, avg_h = get_average_img_resolution(images)\n            st.header(labels[index])\n            st.write(f'Total Samples:  {total_imgs} ')\n            st.write(f'Image Resolution: {avg_w}x{avg_h}')\n            to_show = st.slider('Slide to adjust samples being displayed', 0, total_imgs,\n                                30)\n            st.image(images[:to_show], width=200)","metadata":{"tags":[],"cell_id":"7b6ae4483899422dad0596aa8ca88b52","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":139},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We deployed the Google Teachable Machine on our Streamlit app too. By creating a new method and using the code Google gave us for deploying their model. The model itself is saved to a keras model file that we uploaded to github. Loading the model into a variable is fairly straightforward using the load_model() function. Google Teachable Machine does require us to create another textfile in which we place the labels of our to be predicted classes. Normally Google also gives this file together with the model however we adjusted it slightly to give it the proper names. ","metadata":{"tags":[],"cell_id":"30f7d1d11f21422fb20b416f60875c9f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"def googlemachine():\n    # Disable scientific notation for clarity\n    np.set_printoptions(suppress=True)\n    # Load the model\n    model = load_model(pathlib.Path()/'keras_model.h5', compile=False)\n    # Load the labels\n    class_names = open(pathlib.Path()/'labels.txt', 'r').readlines()\n    # We reshape the image into Google's preferred shape\n    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n    # Utilizing the same file upload as with our image classification\n    google_file = st.file_uploader(\"Upload Files\",type=['png','jpeg', 'jpg'])\n    # Generating two columns again to keep the same layout\n    col1,col2 = st.columns(2)\n    with col1:\n        display_image = st.empty()\n        if not google_file:\n            return  display_image.info(\"Choose a file to upload, only type: png, jpg, jpeg \")\n        else:\n            google_file = PILImage.create((google_file))\n            display_image.image(google_file.to_thumb(500,500), caption='Image Upload')\n    with col2:\n        # We begin with processing the uploaded file as was described with Google's\n        # Teachable Machine\n        image = google_file.convert('RGB')\n        # Resizing the image to a 224x224\n        size = (224, 224)\n        image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n        # Turning the image into a numpy array\n        image_array = np.asarray(image)\n        # Normalizing the image\n        normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n        # Loading the image into the array\n        data[0] = normalized_image_array\n        # Run the inference and predict the processed image\n        prediction = model.predict(data)\n        index = np.argmax(prediction)\n        # Save the predicted class and the probability/confidence score that Google Teachable\n        # Machine gives us\n        class_name = class_names[index]\n        confidence_score = prediction[0][index]\n        # Displaying the predicted class and giving its probability\n        st.success(f'Prediction: {class_name} ')\n        st.info(f'Probability: {confidence_score}')\n    \n    google_file.close()","metadata":{"tags":[],"cell_id":"3dad0d05ab5d4b73b34dec64ee758206","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The last and final tab of our Streamlit app is the new model trainer with fastai. The general idea is the same as our earlier explanations of how we trained our fastai model. Few notable differences however is the addition of 'options' in our app, giving the users the ability to change the learning rate, epochs trained and which resnet to use. resnet152 was not included in these options as the model is far too big to be trained via the Streamlit cloud. Later on it turned out that the dataloaders in general were too memory intensive to be used in the Streamlit cloud and therefore the training of the resnet model has been delegated to only local use. An important change is the slider for the learning rate. Since Streamlit does not allow the base slider values to go down to a certain number, meaning we have to divide the chosen value by a 1000. Upon choosing the required parameters the user will be able to start the training. After the initial training for the amount of epochs and learning rate chosen we will unfreeze the weights and then fine tune the newly trained model once more. Upon the training being finished we export our model as a new pickle file and allow the user to download this pickle file to set up their own prediction service.","metadata":{"tags":[],"cell_id":"21881eb8-0a9e-480f-870b-e37909015bbf","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":58},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"def fastai_training():\n    st.header('New Image Classifier Model')\n    st.subheader('Powered by FastAi')\n    fns = get_image_files('cats/')\n    cats = DataBlock(\n        blocks=(ImageBlock, CategoryBlock), \n        get_items=get_image_files, \n        splitter=RandomSplitter(valid_pct=0.2, seed=1),\n        get_y=parent_label,\n        item_tfms=Resize(128))\n    dls = cats.dataloaders('./cats/')\n    # Random Resize and Augmentation\n    cats = cats.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\n    dls = cats.dataloaders('./cats/')\n    col1, col2 = st.columns(2)\n    col11, col12 = st.columns(2)\n    cnn_arch = col1.selectbox('Select CNN Architecture', options=['resnet50','resnet34']\n    , index = 0)\n    no_epoch = col2.slider('What is your desired number of epochs', min_value=1, max_value=50, \n    value=3, step=1)\n    learning_rate = col11.slider('What is your desired learning rate (Value divided by 1000)'\n    , min_value= 1, max_value=100, value=30, step=10)/1000\n    st.info(f'Calculated learning rate: {learning_rate}')\n    if st.button('Train Model'):\n        resnet_adv = vision_learner(dls, cnn_arch, metrics=error_rate)\n        resnet_adv.fit_one_cycle(no_epoch, learning_rate)\n        resnet_adv.unfreeze()\n        resnet_adv.fit_one_cycle(1, 1e-5)\n        resnet_adv.export('cats.pkl')\n        with open('cats.pkl', 'rb') as f:\n            # Defaults to 'application/octet-stream'\n            st.download_button('Download Model', f, file_name='cats.pkl')  \n    else:\n        st.write('Click on button to start training')","metadata":{"tags":[],"cell_id":"8ae900781d22487293e4da452658dfe0","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Google Teachable Machine","metadata":{"tags":[],"cell_id":"9c8e77794db24acdb2b434c6534a128b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"Google Teachable Machine is a web-based machine learning tool that allows us to easily train machine learning models without needing to have any previous knowledge or experience with machine learning. It provides an easy-to-use interface that allows us to train models using our own images, audio, or video data, and then use those trained models to make predictions on new data. With Teachable Machine, we could choose from a variety of pre-defined model types, such as image classification, sound classification, and pose estimation, and then use the tool to train our own models by providing sample data. Once the model had been trained, We could make our own predictions on the web interface our, as we did, download the Keras model and apply it to our own app.","metadata":{"tags":[],"cell_id":"39f73679-6e99-4658-a795-462ab98f8238","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"\"Benchmark picture\"","metadata":{"tags":[],"cell_id":"62673a4d5e854a6ab3414f12f7d1f5d9","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"The Teachable Machine has pretty good results, yet slightly worse than our model with the advanced options. Even still, the performance of the Teachable Machine ","metadata":{"tags":[],"cell_id":"0c388c71-b012-4bc1-ada5-060d6ece60c1","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Google Vertex","metadata":{"tags":[],"cell_id":"139be2cc69a8417f825725b1a3f02889","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"Google Vertex is a machine learning model developed by Google for image classification. The Google Vertex model is trained using a large dataset of labeled images and uses advanced techniques such as convolutional neural networks to accurately classify images.","metadata":{"tags":[],"cell_id":"d7f7f000-debd-42d5-b7fb-6202ad44eb3d","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"\"Vertex benchmark\"","metadata":{"tags":[],"cell_id":"6fd7c4d1-5051-4b91-bb4b-9e68123877bc","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Vertex used only a handful of images as validation in its training therefore the result should interpreted accordingly. ","metadata":{"tags":[],"cell_id":"44ccbd8b-c03b-4ba7-a22e-3cec9a551445","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### ROC and AUC metric","metadata":{"tags":[],"cell_id":"49f08e94232b4b4f804c321f7bd67a9f","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"cf08cd3bc7d04a1da04ce3a7a9283eb7","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"### Flask API","metadata":{"tags":[],"cell_id":"ffbbdfd8a12048ec901a1a482abe4969","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":61},"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"Flask is a micro web framework written in Python used to create web applications. In this project, we used it to provide an API with a single post method which can make a guess of which feline is shown in an image included in the body of the API call. This image can either be a jpg, jpeg or png file.","metadata":{"tags":[],"cell_id":"c7f43704-715b-4e8b-b849-951316f292ae","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":67},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# necessary imports\nimport os\nfrom flask import Flask, request, make_response, jsonify\nfrom werkzeug.utils import secure_filename\nfrom fastai.vision.all import *\nfrom fastai.data.external import *\nimport pathlib\n\n\ntemp = pathlib.PosixPath\npathlib.PosixPath = pathlib.WindowsPath\n\n# list of allowed extensions\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n\n# initiation\napp = Flask(__name__)\n\n# specify the pickle file made in the BigDataAdvance notebook\nlearner = load_learner('cats.pkl')\n\n# method used to check if the extension of the file matches one of the allowed extensions mentioned earlier\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n# route and method for the API call\n@app.route('/predict', methods=['POST'])\ndef predict():\n    # exception for when there is no image (error code 400)\n    if 'image' not in request.files:\n        return {'error': 'no image found, in request.'}, 400\n\n    # save the image in a variable called file\n    file = request.files['image']\n    # exception for when the name of the file is empty (error code 400)\n    if file.filename == '':\n        return {'error': 'no image found. Empty'}, 400\n \n    # actions for when the file is present and has one of the allowed extensions\n    if file and allowed_file(file.filename): \n        # create an image based on the file\n        img = PILImage.create(file)\n        # let the model make a prediction based on the image\n        pred = learner.predict(img)\n        print(pred)\n        # return \"success\" as well as the prediction (code 200)\n        return {'success': pred[0]}, 200    \n    # create an exception for any other error that may occur (error code 500)\n    return {'error': 'something went wrong.'}, 500\n\nif __name__ == '__main__':\n    # set to port 5000\n    port = os.getenv('PORT',5000)\n    # run the app\n    app.run(debug=True, host='0.0.0.0', port=port) \n    print(\"success\")","metadata":{"tags":[],"cell_id":"2831db744d2b4ecc964dd4e5989fd05b","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":70},"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3ae29049-a1d9-4cc4-a029-77a1751cb9d4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_app_layout":"article","deepnote_notebook_id":"375e9daab4f4476bb426e73afff12019","deepnote_execution_queue":[]}}